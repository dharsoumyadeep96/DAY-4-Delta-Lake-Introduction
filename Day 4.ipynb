{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b90e0123-63ac-4938-9b25-8b6b20d7804e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40e70a43-9be3-4dff-91c8-f2c4fad40ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDEE0️ TASK 1: Convert CSV to Delta Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "090bc558-0214-4035-8da4-8f7855577c3b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n+-----------+-------+-----------+------+\n|employee_id|   name| department|salary|\n+-----------+-------+-----------+------+\n|          1|  Alice|Engineering| 75000|\n|          2|    Bob|      Sales| 65000|\n|          3|Charlie|Engineering| 80000|\n|          4|  David|         HR| 55000|\n|          5|    Eve|      Sales| 70000|\n+-----------+-------+-----------+------+\n\n\n✅ Successfully converted to Delta managed table: employees_delta\n\nData read from Delta Lake managed table:\n+-----------+-------+-----------+------+\n|employee_id|   name| department|salary|\n+-----------+-------+-----------+------+\n|          1|  Alice|Engineering| 75000|\n|          2|    Bob|      Sales| 65000|\n|          3|Charlie|Engineering| 80000|\n|          4|  David|         HR| 55000|\n|          5|    Eve|      Sales| 70000|\n+-----------+-------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# Step 2: Create Spark session with Delta Lake support\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeIntro\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Step 3: Create a sample Spark DataFrame directly\n",
    "data = [\n",
    "    (1, 'Alice', 'Engineering', 75000),\n",
    "    (2, 'Bob', 'Sales', 65000),\n",
    "    (3, 'Charlie', 'Engineering', 80000),\n",
    "    (4, 'David', 'HR', 55000),\n",
    "    (5, 'Eve', 'Sales', 70000)\n",
    "]\n",
    "columns = ['employee_id', 'name', 'department', 'salary']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 4: Show the data\n",
    "print(\"Original data:\")\n",
    "df.show()\n",
    "\n",
    "# Step 5: Write to Delta managed table\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"employees_delta\")\n",
    "print(\"\\n✅ Successfully converted to Delta managed table: employees_delta\")\n",
    "\n",
    "# Step 6: Read back from Delta managed table to verify\n",
    "df_delta = spark.read.table(\"employees_delta\")\n",
    "print(\"\\nData read from Delta Lake managed table:\")\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaeeb5be-2480-49f8-b594-f69ac64d8321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDEE0️ TASK 2: Create Delta Tables (SQL and PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ced70fe5-b21c-4ebb-8e10-b4bddc437e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Method 1: Using PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e648a72-74c2-4766-b47c-cfbae61cc2d0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 5"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta table created using PySpark!\n+----------+------------+-----------+------+\n|product_id|product_name|   category| price|\n+----------+------------+-----------+------+\n|         1|      Laptop|Electronics|999.99|\n|         2|       Mouse|Electronics| 25.99|\n|         3|        Desk|  Furniture|299.99|\n|         4|       Chair|  Furniture|199.99|\n+----------+------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "# Define schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), False),  # False = NOT NULL\n",
    "    StructField(\"product_name\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), True),      # True = NULLABLE\n",
    "    StructField(\"price\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "# Create sample data\n",
    "products_data = [\n",
    "    (1, \"Laptop\", \"Electronics\", 999.99),\n",
    "    (2, \"Mouse\", \"Electronics\", 25.99),\n",
    "    (3, \"Desk\", \"Furniture\", 299.99),\n",
    "    (4, \"Chair\", \"Furniture\", 199.99)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df_products = spark.createDataFrame(products_data, schema)\n",
    "\n",
    "# Write as Delta managed table\n",
    "df_products.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"products_delta\")\n",
    "\n",
    "print(\"✅ Delta table created using PySpark!\")\n",
    "df_products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efd574c2-1773-40c3-a07f-c5cafdc423f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Method 2: Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e564eac-fcd8-4867-a58f-10e338f065a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Delta table created using SQL!\n+----------+------------+-----------+------+\n|product_id|product_name|   category| price|\n+----------+------------+-----------+------+\n|         1|      Laptop|Electronics|999.99|\n|         2|       Mouse|Electronics| 25.99|\n|         3|        Desk|  Furniture|299.99|\n|         4|       Chair|  Furniture|199.99|\n+----------+------------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# First, register the DataFrame as a temp view\n",
    "df_products.createOrReplaceTempView(\"products_temp\")\n",
    "\n",
    "# Create Delta table using SQL (no LOCATION clause)\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE products_delta_sql\n",
    "    USING DELTA\n",
    "    AS SELECT * FROM products_temp\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Delta table created using SQL!\")\n",
    "\n",
    "# Query the table\n",
    "spark.sql(\"SELECT * FROM products_delta_sql\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "802d3b53-394a-4bac-806b-cbcd21350fe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Creating an Empty Table with Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef88fa3-dc4b-4e55-b28c-96918f2abbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Empty Delta table created with schema!\n"
     ]
    }
   ],
   "source": [
    "# SQL approach\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS orders_delta (\n",
    "        order_id INT NOT NULL,\n",
    "        customer_name STRING NOT NULL,\n",
    "        order_date DATE,\n",
    "        total_amount DOUBLE\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ Empty Delta table created with schema!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0edbd88b-9f6b-4e35-a83e-ac29a5c25878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDEE0️ TASK 3: Test Schema Enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b93b784-f9de-4d39-a1c0-216339b24b42",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 12"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Initial table created\n+-------+--------+---+\n|user_id|username|age|\n+-------+--------+---+\n|      1|   alice| 25|\n|      2|     bob| 30|\n+-------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define strict schema\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), False),\n",
    "    StructField(\"username\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Create initial data\n",
    "initial_data = [\n",
    "    (1, \"alice\", 25),\n",
    "    (2, \"bob\", 30)\n",
    "]\n",
    "\n",
    "df_initial = spark.createDataFrame(initial_data, schema)\n",
    "df_initial.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"users_delta\")\n",
    "\n",
    "print(\"✅ Initial table created\")\n",
    "df_initial.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41dd3372-5594-4d96-8443-9b3635f0195a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test 1: Try Adding Data with WRONG Data Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851c0faf-9ef1-462c-a98e-a883088344d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema enforcement worked! Error caught:\n   Path must be absolute: users_delta/_delta_log\n\nJVM stacktrace:\njava.lang.IllegalArgumentException\n\ta...\n"
     ]
    }
   ],
   "source": [
    "# This will FAIL because age is string instead of integer\n",
    "wrong_type_data = [\n",
    "    (3, \"charlie\", \"twenty-five\")  # age as string - WRONG!\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Try to create DataFrame with wrong type\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "    \n",
    "    wrong_schema = StructType([\n",
    "        StructField(\"user_id\", IntegerType(), False),\n",
    "        StructField(\"username\", StringType(), False),\n",
    "        StructField(\"age\", StringType(), False)  # String instead of Int\n",
    "    ])\n",
    "    \n",
    "    df_wrong = spark.createDataFrame(wrong_type_data, wrong_schema)\n",
    "    df_wrong.write.format(\"delta\").mode(\"append\").save(\"users_delta\")\n",
    "    print(\"❌ This shouldn't succeed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"✅ Schema enforcement worked! Error caught:\")\n",
    "    print(f\"   {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db392a64-ac48-4050-879d-9434a68393cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test 2: Try Adding Data with MISSING Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d280552-2606-4841-821c-9abedd30fb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Schema enforcement worked! Error caught:\n   Path must be absolute: users_delta/_delta_log\n\nJVM stacktrace:\njava.lang.IllegalArgumentException\n\ta...\n"
     ]
    }
   ],
   "source": [
    "# This will FAIL because we're missing the 'age' column\n",
    "missing_column_data = [\n",
    "    (4, \"david\")  # Missing age column\n",
    "]\n",
    "\n",
    "try:\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "    \n",
    "    incomplete_schema = StructType([\n",
    "        StructField(\"user_id\", IntegerType(), False),\n",
    "        StructField(\"username\", StringType(), False)\n",
    "        # Missing age field!\n",
    "    ])\n",
    "    \n",
    "    df_incomplete = spark.createDataFrame(missing_column_data, incomplete_schema)\n",
    "    df_incomplete.write.format(\"delta\").mode(\"append\").save(\"users_delta\")\n",
    "    print(\"❌ This shouldn't succeed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"✅ Schema enforcement worked! Error caught:\")\n",
    "    print(f\"   {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8051f8f-413c-47cf-b7c3-b91699e4ccf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test 3: Correctly Add Data (SHOULD WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c67c720-2ac9-4db9-99ba-995cc4a2605c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data appended successfully!\n+-------+--------+---+\n|user_id|username|age|\n+-------+--------+---+\n|      3| charlie| 28|\n|      4|   david| 35|\n|      1|   alice| 25|\n|      2|     bob| 30|\n+-------+--------+---+\n\n"
     ]
    }
   ],
   "source": [
    "# This WILL WORK because schema matches\n",
    "correct_data = [\n",
    "    (3, \"charlie\", 28),\n",
    "    (4, \"david\", 35)\n",
    "]\n",
    "\n",
    "df_correct = spark.createDataFrame(correct_data, schema)\n",
    "df_correct.write.format(\"delta\").mode(\"append\").saveAsTable(\"users_delta\")\n",
    "\n",
    "print(\"✅ Data appended successfully!\")\n",
    "\n",
    "# Verify\n",
    "df_final = spark.read.table(\"users_delta\")\n",
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f267813c-f1c0-49df-9eaa-726f77ee55fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDEE0️ TASK 4: Handle Duplicate Inserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "716a5cd2-aebc-447a-b32c-528e709175bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f56f1682-1606-4ded-9083-1f6606779336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263ac0ab-7920-47de-85eb-5303126eff07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial orders:\n+--------+-------------+----------+------------+\n|order_id|customer_name|order_date|total_amount|\n+--------+-------------+----------+------------+\n|       1|        Alice|2024-01-01|       100.0|\n|       2|          Bob|2024-01-02|       150.0|\n|       3|      Charlie|2024-01-03|       200.0|\n+--------+-------------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create orders table\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_name\", StringType(), False),\n",
    "    StructField(\"order_date\", DateType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "from datetime import date\n",
    "initial_orders = [\n",
    "    (1, \"Alice\", date(2024, 1, 1), 100.00),\n",
    "    (2, \"Bob\", date(2024, 1, 2), 150.00),\n",
    "    (3, \"Charlie\", date(2024, 1, 3), 200.00)\n",
    "]\n",
    "\n",
    "df_orders = spark.createDataFrame(initial_orders, orders_schema)\n",
    "df_orders.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"orders_delta\")\n",
    "\n",
    "print(\"Initial orders:\")\n",
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3005f9c0-a3cf-4eee-8bec-4a032ec0c6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Method 1: Using MERGE (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f155d4f5-e596-4fa1-b482-58a029427407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New orders to insert:\n+--------+-------------+----------+------------+\n|order_id|customer_name|order_date|total_amount|\n+--------+-------------+----------+------------+\n|       2|          Bob|      NULL|       150.0|\n|       4|        David|      NULL|       250.0|\n|       5|          Eve|      NULL|       300.0|\n+--------+-------------+----------+------------+\n\n✅ Merge complete! Duplicates handled:\n+--------+-------------+----------+------------+\n|order_id|customer_name|order_date|total_amount|\n+--------+-------------+----------+------------+\n|       1|        Alice|2024-01-01|       100.0|\n|       2|          Bob|2024-01-02|       150.0|\n|       3|      Charlie|2024-01-03|       200.0|\n|       4|        David|      NULL|       250.0|\n|       5|          Eve|      NULL|       300.0|\n+--------+-------------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# New orders (includes duplicate order_id = 2)\n",
    "new_orders_data = [\n",
    "    (2, \"Bob\", None, 150.00),    # Duplicate - should be ignored\n",
    "    (4, \"David\", None, 250.00),  # New order - should be added\n",
    "    (5, \"Eve\", None, 300.00)     # New order - should be added\n",
    "]\n",
    "\n",
    "df_new_orders = spark.createDataFrame(new_orders_data, orders_schema)\n",
    "\n",
    "print(\"New orders to insert:\")\n",
    "df_new_orders.show()\n",
    "\n",
    "# Load existing Delta table by name (Unity Catalog managed table)\n",
    "delta_table = DeltaTable.forName(spark, \"orders_delta\")\n",
    "\n",
    "# Merge: Insert only if order_id doesn't exist\n",
    "delta_table.alias(\"existing\").merge(\n",
    "    df_new_orders.alias(\"new\"),\n",
    "    \"existing.order_id = new.order_id\"  # Match condition\n",
    ").whenNotMatchedInsertAll().execute()  # Insert only non-matches\n",
    "\n",
    "print(\"✅ Merge complete! Duplicates handled:\")\n",
    "spark.read.table(\"orders_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b8228ed-ecb6-4289-8c5e-f3cde678dd29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Method 2: Using DataFrame Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73eb328c-6b28-43aa-9369-14c4dabc12e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 26"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SQL MERGE complete!\n+--------+-------------+----------+------------+\n|order_id|customer_name|order_date|total_amount|\n+--------+-------------+----------+------------+\n|       1|        Alice|2024-01-01|       100.0|\n|       2|          Bob|2024-01-02|       150.0|\n|       3|      Charlie|2024-01-03|       200.0|\n|       4|        David|      NULL|       250.0|\n|       8|        Henry|      NULL|       450.0|\n|       5|          Eve|      NULL|       300.0|\n|       9|          Ivy|      NULL|       500.0|\n+--------+-------------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Register tables\n",
    "# Minimal fix: use spark.read.table instead of .format('delta').load()\n",
    "df_existing = spark.read.table(\"orders_delta\")\n",
    "df_existing.createOrReplaceTempView(\"existing_orders\")\n",
    "\n",
    "new_orders_data = [\n",
    "    (5, \"Eve\", None, 300.00),      # Duplicate\n",
    "    (8, \"Henry\", None, 450.00),    # New\n",
    "    (9, \"Ivy\", None, 500.00)       # New\n",
    "]\n",
    "\n",
    "df_new = spark.createDataFrame(new_orders_data, orders_schema)\n",
    "df_new.createOrReplaceTempView(\"new_orders\")\n",
    "\n",
    "# SQL MERGE statement\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO orders_delta AS existing\n",
    "    USING new_orders AS new\n",
    "    ON existing.order_id = new.order_id\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "print(\"✅ SQL MERGE complete!\")\n",
    "spark.read.table(\"orders_delta\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 4",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}