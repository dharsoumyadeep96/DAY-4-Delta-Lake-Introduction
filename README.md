# Databricks 14 Days AI Challenge

## ğŸš€ PHASE 1: FOUNDATION (Days 1-4)

DAY 4 (12/01/26)â€“ Delta Lake Introduction

<img width="1080" height="1350" alt="4" src="https://github.com/user-attachments/assets/deff3b94-2649-4dc6-9f9d-3bffa8c3653f" />

This notebook is from the Databricks 14-Day AI Challenge, a hands-on program designed to establish core competencies in Databricks.The Fourth day's objectives is the Basics of  Delta Lake , setting up ACID Transactions , Schema Enforcement , Delta vs Parquet .


ğŸ“Œ  This Challenge is Sponsored by [Databricks](https://www.databricks.com/) and organised by [**Codebasics**](https://codebasics.io/) and [Indian Data Club](https://www.indiandataclub.com/).


### âš™ï¸ Tools & Technologies

* Databricks Community Edition
* Apache Spark (PySpark)
* Spark SQL


### ğŸ“ Assigned Tasks
Convert CSV to Delta âœ…
Create tables in PySpark and SQL âœ…
Enforce and evolve schemas âœ…
Handle duplicate inserts safely âœ…


### ğŸ¯ Key Learning Outcomes
What You Learned Today

Delta Lake Basics

Delta Lake adds reliability and features to Parquet files
Uses transaction logs to track all changes


ACID Transactions

Ensures data consistency and prevents corruption
Safe concurrent reads and writes


Schema Enforcement

Prevents data quality issues
Can evolve schema when needed with mergeSchema option


Delta vs Parquet

Delta = Parquet + ACID + Schema + Time Travel
Better for production data pipelines



